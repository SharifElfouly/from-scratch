{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A vanilla character-level language model RNN\n",
    "\n",
    "Inspired by https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "\n",
    "I will try to generate shakespeare-like text.\n",
    "You can download the .txt file from https://raw.githubusercontent.com/bbejeck/hadoop-algorithms/master/src/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each character has a unique one-hot encoded vector\n",
    "- This vector is the input for the RNN\n",
    "- Output is a vector with the same size for predicting the next char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = open('shakespeare.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "ds, vs = len(data), len(chars)\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "hidden_size = 100\n",
    "seq_len = 25\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "Wxh = np.random.randn(hidden_size, vs)*0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01\n",
    "Why = np.random.randn(vs, hidden_size)*0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(inputs, targets, h_prev):\n",
    "    \"Return the loss, gradients and last model state\"\n",
    "    xs,hs,ys,ps = {},{},{},{}\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        # create one-hot vector\n",
    "        xs[t] = np.zeros((vs,1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][targets[t],0])\n",
    "        \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "        \n",
    "    # backward pass\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # gradient accumalation\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy   += np.dot(dy, hs[t].T)\n",
    "        dby    += dy\n",
    "        dh      = np.dot(Why.T, dy) + dhnext\n",
    "        dhraw   = (1 - hs[t] * hs[t]) * dh\n",
    "        dbh    += dhraw\n",
    "        dWxh   += np.dot(dhraw, xs[t].T)\n",
    "        dWhh   += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext += np.dot(Whh.T, dhraw)\n",
    "        # gradient clipping\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vs, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h  = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y  = np.dot(Why, h) + by\n",
    "        p  = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vs), p=p.ravel())\n",
    "        x = np.zeros((vs, 1))\n",
    "        x [ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = 0,0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)\n",
    "smooth_loss = -np.log(1.0/vs)*seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  no ams of\n",
      "    And of Mot.\n",
      "  MLUCLELORUULERENA.  Thath abe yon oups that heas!\n",
      "    Tal. an mrou,\n",
      "                      cad. Heurs!\n",
      "    Cive thou.,    Orand ig imr enoet iwince. arter thicher, of mary  -----\n",
      "200000, 56.313353006269104\n",
      "210000, 55.58635764879004\n",
      "220000, 59.75383056823793\n",
      "230000, 55.53149624307009\n",
      "240000, 53.17445407567616\n",
      "250000, 53.05513729383428\n",
      "260000, 54.17103746762103\n",
      "270000, 54.2036787607943\n",
      "280000, 54.66659726264434\n",
      "290000, 53.74294854684522\n",
      "---- eot plend pitllends and motte fele?\n",
      "    SAll theily marordin latk\n",
      "                   t's eneps all houl den wach weep mer ding his maal. leir thand ly of thind pord?\n",
      "  LUS.\n",
      "  IMIAG WHERCALES. Sing, yu -----\n",
      "300000, 53.32602972139323\n",
      "310000, 52.76751403318295\n",
      "320000, 52.5123402487749\n",
      "330000, 53.86712508969286\n",
      "340000, 53.02882425142762\n",
      "350000, 51.91591061747107\n",
      "360000, 52.55760828253293\n",
      "370000, 50.91464276547788\n",
      "380000, 52.35740081134182\n",
      "390000, 52.712696773067876\n",
      "---- bom unccost.\n",
      "  ARLINERALO. Bod.\n",
      "  BONRIO. Ay faspalf of\n",
      "    Antants\n",
      "    Whos sor sarerigo.\n",
      "  CRANG SY I ISNY\n",
      "    Theeall ckeme. A'tur'stal elein enselys\n",
      "   whye; Hood the bafr, of End of anquch then m -----\n",
      "400000, 52.818967492831035\n",
      "410000, 51.66403434822723\n",
      "420000, 52.944509841469134\n",
      "430000, 52.66135921404078\n",
      "440000, 52.24758336922776\n",
      "450000, 52.19869955039481\n",
      "460000, 53.05917093425105\n",
      "470000, 50.76806917615877\n",
      "480000, 51.42724106129685\n",
      "490000, 51.927806208448466\n",
      "---- erious enothing heyed. By tighsert\n",
      "  heand, that, horfourath of the ho)st chede forpaed in bealk owe bighar, se yosind of f your mey I in plopth\n",
      "    Shisge\n",
      "  Whereerh hnooningor to the gantl than fon  -----\n",
      "500000, 51.95921993623966\n",
      "510000, 51.549263253959765\n",
      "520000, 50.732148722792\n",
      "530000, 51.179931466438155\n",
      "540000, 50.50066459384982\n",
      "550000, 52.90142523854136\n",
      "560000, 51.19757265694255\n",
      "570000, 52.2259770140211\n",
      "580000, 51.35165875432284\n",
      "590000, 48.69616614012869\n",
      "---- ialg.\n",
      "  LED KIUFUSENOG Dine hiss scuspoiny\n",
      "    Uwen weatme, ?ord'd nof, drast the oot it ot ndads thou inds.\n",
      "  KERTHAMDR WOene, to, grou. Tice hingly co hey the oud sparded dech-be outh'd out game, to -----\n",
      "600000, 50.501049999281854\n",
      "610000, 50.673878085101\n",
      "620000, 51.649700368144195\n",
      "630000, 50.33866666898141\n",
      "640000, 51.23307367557291\n",
      "650000, 50.744521237450435\n",
      "660000, 50.28257297143637\n",
      "670000, 51.10565925782941\n",
      "680000, 53.31705346916214\n",
      "690000, 50.53191002310491\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if p+seq_len+1 >= len(data) or n==0:\n",
    "        # reset RNN memory\n",
    "        hprev = np.zeros((hidden_size, 1))\n",
    "        p = 0\n",
    "    inputs  = [char_to_ix[ch] for ch in data[p:p+seq_len]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_len+1]]\n",
    "    \n",
    "    # sample RNN every 100 step\n",
    "    if n%100000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print(f'---- {txt} -----')\n",
    "        \n",
    "    # forward pass\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, dhprev = loss_fn(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n%10000 == 0: print(f'{n}, {smooth_loss}')\n",
    "        \n",
    "        \n",
    "    # param update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem   += dparam * dparam\n",
    "        param += -lr * dparam / np.sqrt(mem + 1e-8)\n",
    "        \n",
    "    p += seq_len\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not bad for a  super simple vanilla RNN trained in a few minutes!\n",
    "\n",
    "- It pretty quickly plateus at around 50.\n",
    "- In the next step we will do the same with a LSTM and see how much better it is."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
